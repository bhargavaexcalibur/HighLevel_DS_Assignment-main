{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cd7a4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import expr\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"BookEmbeddings1\").getOrCreate()\n",
    "\n",
    "\n",
    "# Read data into RDD using textFile\n",
    "rdd = spark.sparkContext.textFile(\"books_task.csv\")\n",
    "\n",
    "# Use csv.reader to parse each line\n",
    "header = rdd.first()\n",
    "rdd = rdd.filter(lambda line: line != header).map(lambda x: next(csv.reader(StringIO(x))))\n",
    "\n",
    "# Convert RDD to DataFrame\n",
    "columns = [\"Index\",\"Title\", \"description\", \"authors\", \"publisher\", \"publishedDate\", \"categories\", \"Impact\"]\n",
    "df = spark.createDataFrame(rdd, columns)\n",
    "\n",
    "df = df.withColumn(\"text\", expr(\"concat(Title, ' ', description)\"))\n",
    "\n",
    "embeddings_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Define a PySpark UDF to apply the sentence embeddings model\n",
    "def embed_sentences(text):\n",
    "    embeddings = embeddings_model.encode([text])\n",
    "    return embeddings.tolist()[0]\n",
    "\n",
    "# Register the UDF\n",
    "spark.udf.register(\"embed_sentences\", embed_sentences, \"array<double>\")\n",
    "\n",
    "# Apply the UDF to create a new column with embeddings\n",
    "df_with_embeddings = df.withColumn(\"embeddings\", expr(\"embed_sentences(text)\"))\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "df_with_embeddings.select(\"Title\", \"text\", \"embeddings\").show(truncate=False)\n",
    "\n",
    "output_path_for_first_10_rows = \"output5\"\n",
    "df_with_embeddings.write.parquet(output_path_for_first_10_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96ae003",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7da7f00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c207bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f24c83c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1965198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"YourAppName\").getOrCreate()\n",
    "\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "df_sample = spark.read.parquet(\"output5\")\n",
    "\n",
    "\n",
    "df_read_parquet = df_sample.limit(5000)\n",
    "\n",
    "# Assuming you have a variable named embedding_length\n",
    "embedding_length = 384  # Replace with your actual embedding length\n",
    "\n",
    "\n",
    "# Show the DataFrame\n",
    "df_read_parquet.show(truncate=False)\n",
    "\n",
    "# Check if 'Impact' is float and convert it to double\n",
    "df_read_parquet = df_read_parquet.withColumn('Impact', col('Impact').cast('double'))\n",
    "\n",
    "# Check the schema after conversion\n",
    "df_read_parquet.printSchema()\n",
    "\n",
    "# Create separate columns for each dimension of the embeddings\n",
    "for i in range(embedding_length):\n",
    "    col_name = f\"embedding_{i + 1}\"\n",
    "    df_read_parquet = df_read_parquet.withColumn(col_name, df_read_parquet[\"embeddings\"][i])\n",
    "\n",
    "# Check the updated schema\n",
    "df_read_parquet.printSchema()\n",
    "\n",
    "# Select embedding variables and 'Impact' for modeling\n",
    "feature_columns = [f\"embedding_{i + 1}\" for i in range(embedding_length)]\n",
    "feature_columns += ['Impact']\n",
    "\n",
    "df_model = df_read_parquet.select(feature_columns)\n",
    "\n",
    "# Check the statistics of the 'Impact' column\n",
    "df_model.describe('Impact').show()\n",
    "\n",
    "\n",
    "\n",
    "# Train-test split\n",
    "train_df, test_df = df_model.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Prepare feature vector\n",
    "assembler = VectorAssembler(inputCols=feature_columns[:-1], outputCol=\"features\")\n",
    "\n",
    "# Create a Linear Regression model\n",
    "lr = LinearRegression(labelCol=\"Impact\", featuresCol=\"features\")\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline(stages=[assembler, lr])\n",
    "\n",
    "# Define a parameter grid for tuning\n",
    "paramGrid = ParamGridBuilder().addGrid(lr.regParam, [0.0, 0.1, 0.01]).build()\n",
    "\n",
    "# Create a cross-validator\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(labelCol=\"Impact\"),\n",
    "                          numFolds=3)\n",
    "\n",
    "# Fit the model\n",
    "cv_model = crossval.fit(train_df)\n",
    "\n",
    "# Predict on the training set\n",
    "train_predictions = cv_model.transform(train_df)\n",
    "\n",
    "# Predict on the test set\n",
    "test_predictions = cv_model.transform(test_df)\n",
    "\n",
    "# Calculate MAPE for training set\n",
    "train_mape = np.mean(np.abs((train_predictions.select(\"Impact\", \"prediction\").toPandas()[\"Impact\"] -\n",
    "                             train_predictions.select(\"Impact\", \"prediction\").toPandas()[\"prediction\"]) /\n",
    "                            train_predictions.select(\"Impact\", \"prediction\").toPandas()[\"Impact\"])) * 100\n",
    "\n",
    "# Calculate MAPE for test set\n",
    "test_mape = np.mean(np.abs((test_predictions.select(\"Impact\", \"prediction\").toPandas()[\"Impact\"] -\n",
    "                            test_predictions.select(\"Impact\", \"prediction\").toPandas()[\"prediction\"]) /\n",
    "                           test_predictions.select(\"Impact\", \"prediction\").toPandas()[\"Impact\"])) * 100\n",
    "\n",
    "# Print MAPE for training and test sets\n",
    "print(f\"MAPE on training set: {train_mape:.2f}%\")\n",
    "print(f\"MAPE on test set: {test_mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abba774c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887d98b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82c8d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fffe58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c41d33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e960822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4bee47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5facc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1023859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bd49ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d225fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5028b6b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e2c714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d47fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1bab58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba4d5fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada667ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"BookEmbeddings1\").getOrCreate()\n",
    "\n",
    "# Read data directly into DataFrame with appropriate CSV options\n",
    "csv_path = \"books_task.csv\"\n",
    "df = spark.read.option(\"header\", \"true\").option(\"quote\", \"\\\"\").option(\"escape\", \"\\\"\").csv(csv_path)\n",
    "\n",
    "# Rename the '_c0' column to 'Index'\n",
    "df = df.withColumnRenamed(\"_c0\", \"Index\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1f3f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'publishedDate' to date format\n",
    "df = df.withColumn(\"publishedDate\", col(\"publishedDate\").cast(\"date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea6fe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "# Define a UDF to convert string representation of list to actual list\n",
    "@udf(ArrayType(StringType()))\n",
    "def convert_to_list(authors_str):\n",
    "    return (authors_str[1:-1].replace(\"'\", \"\").split(',')) if authors_str is not None else []\n",
    "\n",
    "# Apply the UDF to the 'authors' column\n",
    "df = df.withColumn(\"authors_list\", convert_to_list(col(\"authors\")))\n",
    "\n",
    "df = df.withColumn(\"categories_list\", convert_to_list(col(\"categories\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e116ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d59a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17eb5fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83b9256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values in each column\n",
    "missing_counts = [df.where(col(c).isNull()).count() for c in df.columns]\n",
    "\n",
    "# Create a dictionary to map column names to missing value counts\n",
    "missing_dict = dict(zip(df.columns, missing_counts))\n",
    "\n",
    "# Display the missing value counts for each column\n",
    "for column, missing_count in missing_dict.items():\n",
    "    print(f\"Column '{column}': {missing_count} missing values\")\n",
    "\n",
    "# Alternatively, you can display the missing values in a more tabular format\n",
    "missing_df = spark.createDataFrame([(column, missing_count) for column, missing_count in missing_dict.items()],\n",
    "                                   [\"Column\", \"Missing Values\"])\n",
    "missing_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00eed87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, min, max\n",
    "\n",
    "# Calculate the range of 'publishedDate'\n",
    "date_range = df.agg(min(\"publishedDate\").alias(\"min_date\"), max(\"publishedDate\").alias(\"max_date\")).collect()[0]\n",
    "\n",
    "# Display the range\n",
    "print(\"Range of publishedDate:\")\n",
    "print(f\"Minimum Date: {date_range['min_date']}\")\n",
    "print(f\"Maximum Date: {date_range['max_date']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f15b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a95213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.withColumn('Impact', col('Impact').cast('double'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f0d071",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49560764",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('categories_list').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04df9eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4a7f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# Create a CountVectorizer instance\n",
    "cv = CountVectorizer(inputCol=\"authors_list\", outputCol=\"authors_features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3659f03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cv.fit(df)\n",
    "df_features = model.transform(df)\n",
    "\n",
    "# Display the resulting DataFrame with features\n",
    "df_features.select(\"Index\", \"authors_list\", \"authors_features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e038add1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "# Function to create MultiHot Encoding\n",
    "@udf(VectorUDT())\n",
    "def multi_hot_encoding(authors):\n",
    "    unique_authors = list(set(authors))\n",
    "    encoding = [1.0 if author in unique_authors else 0.0 for author in all_authors]\n",
    "    return Vectors.dense(encoding)\n",
    "\n",
    "# Extract all unique authors\n",
    "all_authors = df.select(\"authors_list\").rdd.flatMap(lambda x: x[0]).distinct().collect()\n",
    "\n",
    "# Apply the MultiHot Encoding UDF\n",
    "df_encoded = df.withColumn(\"multi_hot_encoding\", multi_hot_encoding(\"authors_list\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae26325",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.select('multi_hot_encoding').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72674268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29dd0c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c5011f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d509d66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e430df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b9cb4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11340778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716254a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77034cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2061ff88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40db9c70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38472a82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eedd8a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926a8956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"BookEmbeddings1\").getOrCreate()\n",
    "\n",
    "# Read data directly into DataFrame\n",
    "csv_path = \"books_task.csv\"\n",
    "df = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "\n",
    "df = df.withColumn(\"text\", expr(\"concat(Title, ' ', description)\"))\n",
    "\n",
    "embeddings_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Define a PySpark UDF to apply the sentence embeddings model\n",
    "def embed_sentences(text):\n",
    "    embeddings = embeddings_model.encode([text])\n",
    "    return embeddings.tolist()[0]\n",
    "\n",
    "# Register the UDF\n",
    "spark.udf.register(\"embed_sentences\", embed_sentences, \"array<double>\")\n",
    "\n",
    "# Apply the UDF to create a new column with embeddings\n",
    "df_with_embeddings = df.withColumn(\"embeddings\", expr(\"embed_sentences(text)\"))\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "df_with_embeddings.select(\"Title\", \"text\", \"embeddings\").show(truncate=False)\n",
    "\n",
    "output_path_for_first_10_rows = \"output5\"\n",
    "df_with_embeddings.write.parquet(output_path_for_first_10_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c471cb84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b2f35d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b90b5e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a08d6a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1a8eb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a8f208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "993afd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Setting default log level to \""
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, expr, coalesce, current_date, year, udf\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"BookEmbeddings1\").getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be07afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data directly into DataFrame with appropriate CSV options\n",
    "csv_path = \"books_task.csv\"\n",
    "df = spark.read.option(\"header\", \"true\").option(\"quote\", \"\\\"\").option(\"escape\", \"\\\"\").csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755b98a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = df.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f1eb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rows = len(pandas_df)\n",
    "part1_end = int(total_rows * 0.2)\n",
    "part2_end = int(total_rows * 0.4)\n",
    "part3_end = int(total_rows * 0.6)\n",
    "part4_end = int(total_rows * 0.8)\n",
    "\n",
    "# Split the DataFrame into five parts\n",
    "part1 = pandas_df.iloc[:part1_end, :]\n",
    "part2 = pandas_df.iloc[part1_end:part2_end, :]\n",
    "part3 = pandas_df.iloc[part2_end:part3_end, :]\n",
    "part4 = pandas_df.iloc[part3_end:part4_end, :]\n",
    "part5 = pandas_df.iloc[part4_end:, :]\n",
    "\n",
    "# Save each part to a separate file (e.g., CSV)\n",
    "part1.to_csv('part1.csv', index=False)\n",
    "part2.to_csv('part2.csv', index=False)\n",
    "part3.to_csv('part3.csv', index=False)\n",
    "part4.to_csv('part4.csv', index=False)\n",
    "part5.to_csv('part5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a97e73ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"part1.csv\"\n",
    "df = spark.read.option(\"header\", \"true\").option(\"quote\", \"\\\"\").option(\"escape\", \"\\\"\").csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25ba4def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+----------------+--------------------+-------------+--------------------+-----------------+\n",
      "|_c0|               Title|         description|         authors|           publisher|publishedDate|          categories|           Impact|\n",
      "+---+--------------------+--------------------+----------------+--------------------+-------------+--------------------+-----------------+\n",
      "|  0|Its Only Art If I...|                NULL|['Julie Strain']|Smithsonian Insti...|         1996|['Comics & Graphi...|784.3039243054303|\n",
      "|  1|Dr. Seuss: Americ...|Philip Nel takes ...|  ['Philip Nel']|           A&C Black|   2005-01-01|['Biography & Aut...|825.4655354138016|\n",
      "+---+--------------------+--------------------+----------------+--------------------+-------------+--------------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f666b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- authors: string (nullable = true)\n",
      " |-- publisher: string (nullable = true)\n",
      " |-- publishedDate: string (nullable = true)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- Impact: string (nullable = true)\n",
      "\n",
      "Column 'Index': 0 missing values\n",
      "Column 'Title': 0 missing values\n",
      "Column 'description': 2601 missing values\n",
      "Column 'authors': 590 missing values\n",
      "Column 'publisher': 0 missing values\n",
      "Column 'publishedDate': 80 missing values\n",
      "Column 'categories': 0 missing values\n",
      "Column 'Impact': 0 missing values\n",
      "root\n",
      " |-- Index: string (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- authors: string (nullable = true)\n",
      " |-- publisher: string (nullable = true)\n",
      " |-- publishedDate: string (nullable = true)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- Impact: string (nullable = true)\n",
      "\n",
      "Column 'Index': 0 missing values\n",
      "Column 'Title': 0 missing values\n",
      "Column 'description': 2601 missing values\n",
      "Column 'authors': 590 missing values\n",
      "Column 'publisher': 0 missing values\n",
      "Column 'publishedDate': 0 missing values\n",
      "Column 'categories': 0 missing values\n",
      "Column 'Impact': 0 missing values\n",
      "Column 'Index': 0 missing values\n",
      "Column 'Title': 0 missing values\n",
      "Column 'description': 2601 missing values\n",
      "Column 'authors': 590 missing values\n",
      "Column 'publisher': 0 missing values\n",
      "Column 'publishedDate': 0 missing values\n",
      "Column 'categories': 0 missing values\n",
      "Column 'Impact': 0 missing values\n",
      "Column 'authors_list': 0 missing values\n",
      "Column 'categories_list': 0 missing values\n",
      "root\n",
      " |-- Index: string (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- authors: string (nullable = true)\n",
      " |-- publisher: string (nullable = true)\n",
      " |-- publishedDate: string (nullable = true)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- Impact: string (nullable = true)\n",
      " |-- authors_list: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- categories_list: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "30529\n",
      "4932\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'array_contains' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 111>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m all_categories \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategories_list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mrdd\u001b[38;5;241m.\u001b[39mflatMap(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mdistinct()\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(all_categories)):\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# Use the 'when' function to create a binary column\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, when(\u001b[43marray_contains\u001b[49m(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategories_list\u001b[39m\u001b[38;5;124m\"\u001b[39m), all_categories[i]), \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39motherwise(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    115\u001b[0m df\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    118\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle_word_count\u001b[39m\u001b[38;5;124m\"\u001b[39m, size(split(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'array_contains' is not defined"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "\n",
    "# Rename the '_c0' column to 'Index'\n",
    "df = df.withColumnRenamed(\"_c0\", \"Index\")\n",
    "\n",
    "# Convert 'publishedDate' to date format\n",
    "df = df.withColumn(\"publishedDate\", col(\"publishedDate\").cast(\"date\"))\n",
    "\n",
    "# Count missing values in each column\n",
    "missing_counts = [df.where(col(c).isNull()).count() for c in df.columns]\n",
    "\n",
    "# Create a dictionary to map column names to missing value counts\n",
    "missing_dict = dict(zip(df.columns, missing_counts))\n",
    "\n",
    "# Display the missing value counts for each column\n",
    "for column, missing_count in missing_dict.items():\n",
    "    print(f\"Column '{column}': {missing_count} missing values\")\n",
    "    \n",
    "# Replace missing values with a default date \n",
    "default_date = \"1900-01-01\"\n",
    "df = df.withColumn(\"publishedDate\", when(col(\"publishedDate\").isNull(), default_date).otherwise(col(\"publishedDate\")))\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "# Count missing values in each column\n",
    "missing_counts = [df.where(col(c).isNull()).count() for c in df.columns]\n",
    "\n",
    "# Create a dictionary to map column names to missing value counts\n",
    "missing_dict = dict(zip(df.columns, missing_counts))\n",
    "\n",
    "# Display the missing value counts for each column\n",
    "for column, missing_count in missing_dict.items():\n",
    "    print(f\"Column '{column}': {missing_count} missing values\")\n",
    "    \n",
    "\n",
    "# Define a UDF to convert string representation of list to actual list\n",
    "@udf(ArrayType(StringType()))\n",
    "def convert_to_list(authors_str):\n",
    "    return (authors_str[1:-1].replace(\"'\", \"\").split(',')) if authors_str is not None else []\n",
    "\n",
    "# Apply the UDF to the 'authors' column\n",
    "df = df.withColumn(\"authors_list\", convert_to_list(col(\"authors\")))\n",
    "\n",
    "# Apply the UDF to the 'categories' column\n",
    "df = df.withColumn(\"categories_list\", convert_to_list(col(\"categories\")))\n",
    "\n",
    "\n",
    "#Count missing values in each column\n",
    "missing_counts = [df.where(col(c).isNull()).count() for c in df.columns]\n",
    "\n",
    "# Create a dictionary to map column names to missing value counts\n",
    "missing_dict = dict(zip(df.columns, missing_counts))\n",
    "\n",
    "# Display the missing value counts for each column\n",
    "for column, missing_count in missing_dict.items():\n",
    "    print(f\"Column '{column}': {missing_count} missing values\")\n",
    "    \n",
    "df.printSchema()\n",
    "\n",
    "# Extract all unique authors\n",
    "all_authors = df.select(\"authors_list\").rdd.flatMap(lambda x: x[0]).distinct().collect()\n",
    "\n",
    "# Apply the MultiHot Encoding UDF\n",
    "print(len(all_authors))\n",
    "\n",
    "\n",
    "# Print unique publishers\n",
    "unique_publishers = df.select(\"publisher\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "print(len(unique_publishers))\n",
    "\n",
    "\n",
    "df = df.withColumn(\"text\", coalesce(col(\"Title\"), col(\"description\")))\n",
    "\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "df = df.withColumn(\"publishedYear\", year(\"publishedDate\"))\n",
    "\n",
    "# calculate the age\n",
    "df = df.withColumn(\"age\", year(current_date()) - year(\"publishedDate\")) \n",
    "\n",
    "\n",
    "df = df.withColumn(\"Impact\", col(\"Impact\").cast(\"double\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "embeddings_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "def embed_sentences(text):\n",
    "    embeddings = embeddings_model.encode([text])\n",
    "    return embeddings.tolist()[0]\n",
    "\n",
    "# Register the UDF\n",
    "spark.udf.register(\"embed_sentences\", embed_sentences, \"array<double>\")\n",
    "\n",
    "# Apply the UDF to create a new column with embeddings\n",
    "df = df.withColumn(\"text_embeddings\", expr(\"embed_sentences(text)\"))\n",
    "\n",
    "\n",
    "embedding_length= 384\n",
    "\n",
    "# Create separate columns for each dimension of the text_embeddings\n",
    "for i in range(embedding_length):\n",
    "        col_name = f\"text_embeddings{i + 1}\"\n",
    "        df = df.withColumn(col_name, df[\"text_embeddings\"][i])\n",
    "\n",
    "all_categories = df.select(\"categories_list\").rdd.flatMap(lambda x: x[0]).distinct().collect()\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(all_categories)):\n",
    "    # Use the 'when' function to create a binary column\n",
    "    df = df.withColumn(f\"category_{i}\", when(array_contains(col(\"categories_list\"), all_categories[i]), 1).otherwise(0))\n",
    "\n",
    "df.show(2)\n",
    "\n",
    "\n",
    "df = df.withColumn(\"title_word_count\", size(split(col(\"Title\"), \" \")))\n",
    "df = df.withColumn(\"title_avg_word_length\", length(col(\"Title\")) / col(\"title_word_count\"))\n",
    "df = df.withColumn(\"char_count\", length(col(\"Title\")))\n",
    "df = df.withColumn(\"contains_digits\", when(col(\"Title\").rlike(\"\\\\d\"), 1).otherwise(0))\n",
    "df = df.withColumn(\"contains_uppercase\", when(col(\"Title\").rlike(\"[A-Z]\"), 1).otherwise(0))\n",
    "df = df.withColumn(\"contains_punctuation\", when(col(\"Title\").rlike(\"[^\\w\\s]\"), 1).otherwise(0))\n",
    "\n",
    "\n",
    "# Define a UDF for sentiment analysis using TextBlob\n",
    "def analyze_sentiment(title):\n",
    "    blob = TextBlob(title)\n",
    "    return blob.sentiment.polarity\n",
    "\n",
    "sentiment_udf = udf(analyze_sentiment, StringType())\n",
    "\n",
    "# Apply the UDF to the 'Title' column\n",
    "df = df.withColumn(\"sentiment_score\", sentiment_udf(col(\"Title\")))\n",
    "\n",
    "df = df.withColumn(\"sentiment_score\", col(\"sentiment_score\").cast(\"double\"))\n",
    "\n",
    "columns_to_drop = ['description','categories_list', 'categories','publishedDate', 'publishedYear','authors_list', 'authors','Title','text','text_embeddings']\n",
    "df = df.drop(*columns_to_drop)\n",
    "\n",
    "# Coalesce the DataFrame to a single partition\n",
    "df_single_partition = df.coalesce(1)\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "df_single_partition.write.csv(\"output_file1.csv\", header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5aa138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea77528d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd3da20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e15160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a73186b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8178b25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adf9e35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deb7108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45efd5a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0476aa43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d09e276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadbd49d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f704e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc51a23a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94a6cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Specify the folder path to be zipped\n",
    "folder_path = 'output5'\n",
    "\n",
    "# Specify the output zip file path and name\n",
    "output_zip_path = 'output6'\n",
    "\n",
    "# Create a zip archive of the folder\n",
    "shutil.make_archive(output_zip_path, 'zip', folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e138c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ead123",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Start MLflow run\n",
    "with mlflow.start_run(run_name=\"LinearRegression_PCA\"):\n",
    "\n",
    "    # Read the Parquet file into a DataFrame\n",
    "    df_sample = spark.read.parquet(\"/content/output\")\n",
    "    df_read_parquet = df_sample.limit(1000)\n",
    "\n",
    "    # Assuming you have a variable named embedding_length\n",
    "    embedding_length = 384  # Replace with your actual embedding length\n",
    "\n",
    "    # Check if 'Impact' is float and convert it to double\n",
    "    df_sample = df_read_parquet.withColumn('Impact', col('Impact').cast('double'))\n",
    "\n",
    "    # Select embedding variables and 'Impact' for modeling\n",
    "    feature_columns = [f\"text_embeddings{i + 1}\" for i in range(embedding_length)]\n",
    "    df_model = df_sample.select(feature_columns + ['Impact'])\n",
    "\n",
    "    # Use PCA for dimensionality reduction to 50 components\n",
    "    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "    pca = PCA(k=50, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "\n",
    "    # Train-test split\n",
    "    train_df, test_df = df_model.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "    # Create a Linear Regression model\n",
    "    lr = LinearRegression(labelCol=\"Impact\", featuresCol=\"pca_features\")\n",
    "\n",
    "    # Create a pipeline\n",
    "    pipeline = Pipeline(stages=[assembler, pca, lr])\n",
    "\n",
    "    # Define a parameter grid for tuning\n",
    "    paramGrid = ParamGridBuilder().addGrid(lr.regParam, [0.0, 0.1, 0.01]).build()\n",
    "\n",
    "    # Create a cross-validator\n",
    "    crossval = CrossValidator(estimator=pipeline,\n",
    "                              estimatorParamMaps=paramGrid,\n",
    "                              evaluator=RegressionEvaluator(labelCol=\"Impact\"),\n",
    "                              numFolds=3)\n",
    "\n",
    "    # Fit the model\n",
    "    cv_model = crossval.fit(train_df)\n",
    "\n",
    "    # Log parameters\n",
    "    mlflow.log_params({\n",
    "        \"regParam\": cv_model.bestModel.stages[2].getRegParam(),\n",
    "        \"elasticNetParam\": cv_model.bestModel.stages[2].getElasticNetParam()\n",
    "    })\n",
    "\n",
    "    # Log metrics\n",
    "    train_predictions = cv_model.transform(train_df)\n",
    "    test_predictions = cv_model.transform(test_df)\n",
    "\n",
    "    # Calculate MAPE for training set\n",
    "    train_mape = train_predictions.withColumn('mape', F.abs((col('Impact') - col('prediction')) / col('Impact')) * 100)\n",
    "    training_mape = train_mape.select(F.mean('mape')).collect()[0][0]\n",
    "    mlflow.log_metric(\"training_mape\", training_mape)\n",
    "\n",
    "    # Calculate MAPE for test set\n",
    "    test_mape = test_predictions.withColumn('mape', F.abs((col('Impact') - col('prediction')) / col('Impact')) * 100)\n",
    "    test_mape = test_mape.select(F.mean('mape')).collect()[0][0]\n",
    "    mlflow.log_metric(\"test_mape\", test_mape)\n",
    "\n",
    "    # Log the model\n",
    "    mlflow.spark.log_model(cv_model.bestModel, \"model\")\n",
    "\n",
    "    # Show MAPE on training set\n",
    "    print(f\"MAPE on training set: {training_mape:.2f}%\")\n",
    "\n",
    "    # Show MAPE on test set\n",
    "    print(f\"MAPE on test set: {test_mape:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
